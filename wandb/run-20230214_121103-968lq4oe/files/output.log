
Epoch:   0%|                                                                                                                  | 0/10 [00:00<?, ?it/s]
0 0 	| 1.929 	| 0.0938
0 1 	| 1.911 	| 0.125
0 2 	| 1.9108 	| 0.0938
0 3 	| 1.7735 	| 0.1562
0 4 	| 1.862 	| 0.2812
0 5 	| 1.8457 	| 0.3125
0 6 	| 1.6087 	| 0.3438
0 7 	| 1.6116 	| 0.2188
0 8 	| 1.6729 	| 0.25
0 9 	| 1.5765 	| 0.3125
0 10 	| 1.6208 	| 0.25
0 11 	| 1.595 	| 0.375
0 12 	| 1.4408 	| 0.3125
0 13 	| 1.8032 	| 0.2812
0 14 	| 1.7247 	| 0.1875
0 15 	| 1.5373 	| 0.1875
0 16 	| 1.6823 	| 0.1875
0 17 	| 1.6593 	| 0.1562
0 18 	| 1.7699 	| 0.2188
0 19 	| 1.6605 	| 0.25
0 20 	| 1.6557 	| 0.2188
0 21 	| 1.4618 	| 0.2812
0 22 	| 1.711 	| 0.125
0 23 	| 1.6924 	| 0.1562
0 24 	| 1.5928 	| 0.25
0 25 	| 1.5417 	| 0.2188
0 26 	| 1.6219 	| 0.2188
0 27 	| 1.4567 	| 0.3125
0 28 	| 1.9184 	| 0.125
0 29 	| 1.7052 	| 0.1875
0 30 	| 1.6112 	| 0.2188
0 31 	| 1.763 	| 0.125
0 32 	| 1.5947 	| 0.1562
0 33 	| 1.3914 	| 0.375
0 34 	| 1.5418 	| 0.2812
0 35 	| 1.7013 	| 0.2188
0 36 	| 1.4354 	| 0.3125
0 37 	| 1.3997 	| 0.2812
0 38 	| 1.429 	| 0.4062
0 39 	| 1.549 	| 0.4062
0 40 	| 1.5691 	| 0.2188
0 41 	| 1.8704 	| 0.1562
0 42 	| 1.3016 	| 0.4375
0 43 	| 1.6319 	| 0.2188
0 44 	| 1.469 	| 0.25
0 45 	| 1.7066 	| 0.2812
0 46 	| 1.5312 	| 0.3438
0 47 	| 1.4964 	| 0.3438
0 48 	| 1.6356 	| 0.2812
0 49 	| 1.4535 	| 0.2812
0 50 	| 1.5765 	| 0.375
0 51 	| 1.7246 	| 0.2812
0 52 	| 1.5111 	| 0.2812
0 53 	| 1.5147 	| 0.1875
0 54 	| 1.2599 	| 0.4375
0 55 	| 1.2275 	| 0.5
0 56 	| 1.5096 	| 0.25
0 57 	| 1.5789 	| 0.3438
0 58 	| 1.5745 	| 0.3125
0 59 	| 1.4941 	| 0.3438
0 60 	| 1.5574 	| 0.3438
0 61 	| 1.5319 	| 0.2188
0 62 	| 1.2258 	| 0.4375
0 63 	| 1.2881 	| 0.3125
0 64 	| 1.4804 	| 0.2812
0 65 	| 1.5419 	| 0.1875
0 66 	| 1.5537 	| 0.2188
0 67 	| 1.6273 	| 0.3125
0 68 	| 1.5005 	| 0.3125
0 69 	| 1.6057 	| 0.1875
0 70 	| 1.712 	| 0.2188
0 71 	| 1.1632 	| 0.5
0 72 	| 1.3877 	| 0.4062
0 73 	| 1.4173 	| 0.3125
Epoch:   0%|                                                                                                                  | 0/10 [00:46<?, ?it/s]
Traceback (most recent call last):
  File "/home/sebastiaan/fun/text_geolocation/main.py", line 34, in <module>
    main()
  File "/home/sebastiaan/fun/text_geolocation/main.py", line 30, in main
    train(model, train_loader, eval_loader, config["epochs"], config["lr"])
  File "/home/sebastiaan/fun/text_geolocation/train.py", line 84, in train
    eval_mae_km, mean_eval_loss = validation_step(model, eval_loader)
  File "/home/sebastiaan/fun/text_geolocation/train.py", line 118, in validation_step
    eval_output = model(input_ids, attention_mask)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1212, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/sebastiaan/fun/text_geolocation/model.py", line 36, in forward
    roberta_output = self.pretrained_model(input_ids, attention_mask)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 1221, in forward
    outputs = self.roberta(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 853, in forward
    encoder_outputs = self.encoder(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 527, in forward
    layer_outputs = layer_module(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 454, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/pytorch_utils.py", line 249, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 466, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/activations.py", line 57, in forward
    return self.act(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 11.75 GiB total capacity; 11.24 GiB already allocated; 13.50 MiB free; 11.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/sebastiaan/fun/text_geolocation/main.py", line 34, in <module>
    main()
  File "/home/sebastiaan/fun/text_geolocation/main.py", line 30, in main
    train(model, train_loader, eval_loader, config["epochs"], config["lr"])
  File "/home/sebastiaan/fun/text_geolocation/train.py", line 84, in train
    eval_mae_km, mean_eval_loss = validation_step(model, eval_loader)
  File "/home/sebastiaan/fun/text_geolocation/train.py", line 118, in validation_step
    eval_output = model(input_ids, attention_mask)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1212, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/sebastiaan/fun/text_geolocation/model.py", line 36, in forward
    roberta_output = self.pretrained_model(input_ids, attention_mask)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 1221, in forward
    outputs = self.roberta(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 853, in forward
    encoder_outputs = self.encoder(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 527, in forward
    layer_outputs = layer_module(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 454, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/pytorch_utils.py", line 249, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 466, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sebastiaan/.local/lib/python3.10/site-packages/transformers/activations.py", line 57, in forward
    return self.act(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 11.75 GiB total capacity; 11.24 GiB already allocated; 13.50 MiB free; 11.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF